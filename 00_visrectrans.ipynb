{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp visrectrans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VisRecTrans\n",
    "\n",
    "> A class for creating a custom [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929) model for visual recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastai.vision.all import *\n",
    "import timm\n",
    "import math \n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "#hide\n",
    "# Heavily inspired by \"https://github.com/rwightman/pytorch-image-models/blob/5f9aff395c224492e9e44248b15f44b5cc095d9c/timm/models/vision_transformer.py\"\n",
    "\n",
    "class EmbedBlock (Module) :\n",
    "  def __init__ (self, num_patches, embed_dim) :\n",
    "    self.cls_tokens = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "    self.pos_embeds = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "\n",
    "  def forward (self, x) :\n",
    "    B = x.shape[0]\n",
    "    cls_tokens = self.cls_tokens.expand(B, -1, -1)\n",
    "    x = torch.cat((cls_tokens, x), dim = 1)\n",
    "    x = x + self.pos_embeds\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "#hide\n",
    "# Heavily inspired by \"https://github.com/rwightman/pytorch-image-models/blob/5f9aff395c224492e9e44248b15f44b5cc095d9c/timm/models/vision_transformer.py\"\n",
    "\n",
    "class Header (Module) :\n",
    "  def __init__ (self, ni, num_classes) :\n",
    "    self.head = nn.Linear(ni, num_classes)\n",
    "  \n",
    "  def forward (self, x) :\n",
    "    x = x[:, 0]                  # Extracting the clsass token, which is used for the classification task. \n",
    "    x = self.head(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "#hide\n",
    "def custom_ViT (timm_model_name, num_patches, embed_dim, ni, num_classes, pretrained = True) :\n",
    "  model = timm.create_model(timm_model_name, pretrained) \n",
    "  module_layers = list(model.children())\n",
    "  return nn.Sequential(\n",
    "      module_layers[0],\n",
    "      EmbedBlock(num_patches, embed_dim),\n",
    "      nn.Sequential(*module_layers[1:-1]),\n",
    "      Header(ni, num_classes)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "#hide\n",
    "# Heavily inspired by \"https://github.com/rwightman/pytorch-image-models/blob/5f9aff395c224492e9e44248b15f44b5cc095d9c/timm/models/vision_transformer.py\"\n",
    "\n",
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "def trunc_normal_(layer, param, mean=0., std=1., a=-2., b=2.):\n",
    "    # type : (Tensor, float, float, float, float) -> Tensor\n",
    "    \"\"\"Fills the input Tensor with values drawn from a truncated\n",
    "    normal distribution. The values are effectively drawn from the\n",
    "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
    "    with values outside :math:`[a, b]` redrawn until they are within\n",
    "    the bounds. The method used for generating the random values works\n",
    "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
    "    Args:\n",
    "        tensor: an n-dimensional `torch.Tensor`\n",
    "        mean: the mean of the normal distribution\n",
    "        std: the standard deviation of the normal distribution\n",
    "        a: the minimum cutoff value\n",
    "        b: the maximum cutoff value\n",
    "    Examples:\n",
    "        >>> w = torch.empty(3, 5)\n",
    "        >>> nn.init.trunc_normal_(w)\n",
    "    \"\"\"\n",
    "    tensor = layer.get_parameter(param)\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VisRecTrans :\n",
    "    \"\"\"Class for setting up a vision transformer for visual recognition.\n",
    "    Returns a pretrained custom ViT model for the given `model_name` and `num_classes`, by default, or, with randomly initialized parameters, if `pretrained`\n",
    "    is set to False.\n",
    "    \"\"\"\n",
    "\n",
    "    models_list = ['vit_large_patch16_224', 'vit_large_patch16_224_in21k', 'vit_huge_patch14_224_in21k', 'vit_small_patch16_224', 'vit_small_patch16_224_in21k']\n",
    "    # Two tasks : (1) Generalize the assignments of num_path (2) (3) ()\n",
    "    def __init__(self, model_name, num_classes, pretrained = True) :\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.pretrained = pretrained\n",
    "        if self.model_name == 'vit_small_patch16_224' :\n",
    "            self.num_patches = 196\n",
    "            self.embed_dim = 384\n",
    "            self.ni = 384 \n",
    "\n",
    "        elif self.model_name == 'vit_small_patch16_224_in21k' :\n",
    "            self.num_patches = 196\n",
    "            self.embed_dim = 384\n",
    "            self.ni = 384 \n",
    "\n",
    "        elif self.model_name == 'vit_large_patch16_224' :\n",
    "            self.num_patches = 196\n",
    "            self.embed_dim = 1024\n",
    "            self.ni = 1024\n",
    "\n",
    "        elif self.model_name == 'vit_large_patch16_224_in21k' :\n",
    "            self.num_patches = 196\n",
    "            self.embed_dim = 1024\n",
    "            self.ni = 1024\n",
    "\n",
    "        elif self.model_name == 'vit_huge_patch14_224_in21k' :\n",
    "            self.num_patches = 256\n",
    "            self.embed_dim = 1280\n",
    "            self.ni = 1280\n",
    "    \n",
    "    def create_model (self) :\n",
    "        \"\"\"Method for creating the model.\n",
    "        \"\"\"\n",
    "        return custom_ViT(self.model_name, self.num_patches, self.embed_dim, self.ni, self.num_classes, self.pretrained)    \n",
    "\n",
    "    def initialize (self, model) :\n",
    "        \"\"\"Mthod for initializing the given `model`. This method uses truncated normal distribution for \n",
    "        initializing the position embedding as well as the class token, and, the head of the model is \n",
    "        initialized using He initialization.\n",
    "        \"\"\"\n",
    "        trunc_normal_(model[1], 'cls_tokens')\n",
    "        trunc_normal_(model[1], 'pos_embeds')\n",
    "        apply_init(model[3], nn.init.kaiming_normal_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"VisRecTrans.create_model\" class=\"doc_header\"><code>VisRecTrans.create_model</code><a href=\"__main__.py#L39\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>VisRecTrans.create_model</code>()\n",
       "\n",
       "Method for creating the model.\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(VisRecTrans.create_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"VisRecTrans.initialize\" class=\"doc_header\"><code>VisRecTrans.initialize</code><a href=\"__main__.py#L44\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>VisRecTrans.initialize</code>(**`model`**)\n",
       "\n",
       "Mthod for initializing the given `model`. This method uses truncated normal distribution for \n",
       "initializing the position embedding as well as the class token, and, the head of the model is \n",
       "initialized using He initialization."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(VisRecTrans.initialize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if this class is working well :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_rec_ob = VisRecTrans('vit_small_patch16_224', 10, False)\n",
    "model_test = vis_rec_ob.create_model()\n",
    "vis_rec_ob.initialize(model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(model_test, nn.Sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the model is a sequential list of layers, and can be used with the `Learner` class of [fastai](https://docs.fast.ai), as we use any other model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The list of models supported by the `VisRecTrans` class : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vit_large_patch16_224',\n",
       " 'vit_large_patch16_224_in21k',\n",
       " 'vit_huge_patch14_224_in21k',\n",
       " 'vit_small_patch16_224',\n",
       " 'vit_small_patch16_224_in21k']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VisRecTrans.models_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
